{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표본내 성능과 표본외 성능\n",
    "- 학습데이터 집합의 종속 변수값을 얼마나 잘 예측했는지 나타내는 성능을 표본내 성능이라고 한다.\n",
    "- 학습에 쓰이지 않는 표본 데이터 집합의 종속 변수값을 얼마나 잘 예측했는지 나타내는 성능을 표본외성능 혹은 교차검증이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과최적화\n",
    "- 표본내 성능은 좋으면서 표본외 성능이 상대적으로 많이 떨어지면 과최적화라고한다.\n",
    "- 이러한 과최적화가 발생하는 것을 탐지하기 위한 교차검증 방법을 공부한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodels 패키지의 교차검증\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()\n",
    "dfX = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])\n",
    "df = pd.concat([dfX, dfy], axis=1)\n",
    "\n",
    "N = len(df)\n",
    "ratio = 0.7\n",
    "np.random.seed(0)\n",
    "idx_train = np.random.choice(np.arange(N), np.int(ratio * N))\n",
    "idx_test = list(set(np.arange(N)).difference(idx_train))\n",
    "\n",
    "df_train = df.iloc[idx_train]\n",
    "df_test = df.iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.757\n",
      "Model:                            OLS   Adj. R-squared:                  0.747\n",
      "Method:                 Least Squares   F-statistic:                     81.31\n",
      "Date:                Sun, 04 Jul 2021   Prob (F-statistic):           7.22e-96\n",
      "Time:                        19:08:13   Log-Likelihood:                -1057.6\n",
      "No. Observations:                 354   AIC:                             2143.\n",
      "Df Residuals:                     340   BIC:                             2197.\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     40.6105      6.807      5.966      0.000      27.222      53.999\n",
      "CRIM          -0.0801      0.040     -2.012      0.045      -0.158      -0.002\n",
      "ZN             0.0438      0.016      2.777      0.006       0.013       0.075\n",
      "INDUS          0.0978      0.076      1.287      0.199      -0.052       0.247\n",
      "CHAS           2.7905      1.120      2.491      0.013       0.587       4.994\n",
      "NOX          -21.4614      4.919     -4.363      0.000     -31.136     -11.787\n",
      "RM             3.7948      0.532      7.128      0.000       2.748       4.842\n",
      "AGE            0.0006      0.016      0.036      0.971      -0.030       0.031\n",
      "DIS           -1.6910      0.256     -6.605      0.000      -2.195      -1.187\n",
      "RAD            0.2730      0.079      3.447      0.001       0.117       0.429\n",
      "TAX           -0.0097      0.004     -2.215      0.027      -0.018      -0.001\n",
      "PTRATIO       -1.1651      0.167     -6.983      0.000      -1.493      -0.837\n",
      "B              0.0134      0.004      3.815      0.000       0.006       0.020\n",
      "LSTAT         -0.5490      0.062     -8.908      0.000      -0.670      -0.428\n",
      "==============================================================================\n",
      "Omnibus:                      129.426   Durbin-Watson:                   1.979\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              603.605\n",
      "Skew:                           1.498   Prob(JB):                    8.49e-132\n",
      "Kurtosis:                       8.652   Cond. No.                     1.64e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.64e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(boston.feature_names), data=df_train)\n",
    "result = model.fit()\n",
    "print(result.summary()) # 학습용 데이터의 결정계수는 0.757이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.688373412498711"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = result.predict(df_test)\n",
    "\n",
    "rss = ((df_test.MEDV - pred) ** 2).sum()\n",
    "tss = ((df_test.MEDV - df_test.MEDV.mean())** 2).sum()\n",
    "rsquared = 1 - rss / tss\n",
    "rsquared # 검증용 데이터의 결정계수는 0.688이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn의 교차검증 기능\n",
    "- trin_test_split(data, data2, test_size, train_size, random_state)\n",
    "- data : 독립변수 데이터\n",
    "- data2 : 종속변수 데이터\n",
    "- test_size : 검증용 데이터 개수\n",
    "- train_size : 학습용 데이터 개수\n",
    "- random_state : 난수 시드\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 14), (152, 14))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size = 0.3, random_state  = 0)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 13), (354, 1), (152, 13), (152, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfX_train, dfX_test, dfy_train, dfy_test = train_test_split(dfX, dfy, test_size=0.3, random_state=0)\n",
    "dfX_train.shape, dfy_train.shape, dfX_test.shape, dfy_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-폴드 교차검증\n",
    "- 데이터의 수가 적은 경우에는 검증 데이터의 수도 적기 때문에 검증 성능의 신뢰도가 떨어진다. 그렇다고 검증 데이터의 수를 늘리면 테스트 데이터의 수가 적어지므로\n",
    "정상적인 학습이 되지 않는다.\n",
    "- 이러한 딜레마를 해결하기 위한 검증 방법이 K-폴드 교차검증 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 R2 = 0.77301356, 검증 R2 = 0.58922238\n",
      "학습 R2 = 0.72917058, 검증 R2 = 0.77799144\n",
      "학습 R2 = 0.74897081, 검증 R2 = 0.66791979\n",
      "학습 R2 = 0.75658611, 검증 R2 = 0.66801630\n",
      "학습 R2 = 0.70497483, 검증 R2 = 0.83953317\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scores = np.zeros(5)\n",
    "cv = KFold(5, shuffle=True, random_state=0)\n",
    "for i, (idx_train, idx_test) in enumerate(cv.split(df)):\n",
    "    df_train = df.iloc[idx_train]\n",
    "    df_test = df.iloc[idx_test]\n",
    "    \n",
    "    model = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(boston.feature_names), data=df_train)\n",
    "    result = model.fit()\n",
    "    \n",
    "    pred = result.predict(df_test)\n",
    "    rss = ((df_test.MEDV - pred) ** 2).sum()\n",
    "    tss = ((df_test.MEDV - df_test.MEDV.mean())** 2).sum()\n",
    "    rsquared = 1 - rss / tss\n",
    "    \n",
    "    scores[i] = rsquared\n",
    "    print(\"학습 R2 = {:.8f}, 검증 R2 = {:.8f}\".format(result.rsquared, rsquared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58922238, 0.77799144, 0.66791979, 0.6680163 , 0.83953317])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "scores = np.zeros(5)\n",
    "cv = KFold(5, shuffle=True, random_state=0)\n",
    "for i, (idx_train, idx_test) in enumerate(cv.split(df)):\n",
    "    df_train = df.iloc[idx_train]\n",
    "    df_test = df.iloc[idx_test]\n",
    "    \n",
    "    model = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(boston.feature_names), data=df_train)\n",
    "    result = model.fit()\n",
    "    \n",
    "    pred = result.predict(df_test)\n",
    "    rsquared = r2_score(df_test.MEDV, pred)\n",
    "    \n",
    "    scores[i] = rsquared\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차검증 반복\n",
    "- 위와 같이 교차검증을 반복하는 코드를 더 간단하게 만들어주는 함수가 있다. 바로 cross_val_score이다.\n",
    "- cross_val_score(model, X, y, scoring = None, cv = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "class StatsmodelsOLS(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, formula):\n",
    "        self.formula = formula\n",
    "        self.model = None\n",
    "        self.data = None\n",
    "        self.result = None\n",
    "        \n",
    "    def fit(self, dfX, dfy):\n",
    "        self.data = pd.concat([dfX, dfy], axis=1)\n",
    "        self.model = smf.ols(self.formula, data=self.data)\n",
    "        self.result = self.model.fit()\n",
    "        \n",
    "    def predict(self, new_data):\n",
    "        return self.result.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58922238, 0.77799144, 0.66791979, 0.6680163 , 0.83953317])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = StatsmodelsOLS(\"MEDV ~ \" + \"+\".join(boston.feature_names))\n",
    "cv = KFold(5, shuffle=True, random_state=0)\n",
    "cross_val_score(model, dfX, dfy, scoring=\"r2\", cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
